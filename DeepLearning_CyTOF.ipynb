{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1> Diagnose Latent Cytomegalovirus Using Deep Learning <h1> </center>\n",
    "\n",
    "<center>Zicheng Hu, Ph.D.</center>\n",
    "<center>Research Scientist</center>\n",
    "<center>ImmPort Team</center>\n",
    "<center>The Unversity of California, San Francisco</center>\n",
    "\n",
    "![alt text](Data/header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "A deep neural network (a.k.a. deep learning) is an artificial neural network with multiple layers between the input and output layers. It was proven to be highly effective for a variety of predictive tasks. In health care, deep learning is quickly gaining popularity and has been implemented for applications such as image-based diagnosis and personalized drug recommendations. In this tutorial, we will build a tailored deep-learning model for CyTOF data to diagnosis latent Cytomegalovirus infection. \n",
    "\n",
    "We will achieve the following goals:\n",
    "1. Construct a deep learning model tailored to cytometry data using **keras**, a popular deep learning framework.\n",
    "1. Apply the deep learning model to diagnose latent cytomegalovirus (CMV) infection.\n",
    "1. Learn basic python commands for data analysis and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import Functions\n",
    "Before we start, we first import functions that we will use in this tutorial from different libraries. \n",
    "\n",
    "* **keras**, a library the handles the user-end of deep learning. \n",
    "* **tensorflow**, a library that handles the back-end of deep learning.\n",
    "* **pickle**, a library for loading data.\n",
    "* **pandas**, a library for manipulating data. \n",
    "* **random**, a library for generating random numbers. \n",
    "* **numpy**, a library for matrix computation. \n",
    "* **matplotlib**, a library for data visualization\n",
    "* **sklearn**, a library that provides the infrastructure of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "##### Step 1: import functions #####\n",
    "from keras.layers import Dense, Flatten, BatchNormalization, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D, Input\n",
    "from keras.models import load_model, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import seed; seed(111)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow import set_random_seed; set_random_seed(111)\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load data\n",
    "We then load the data, which are stored in the \"allData.obj\" file. For the convenience of this tutorial, the data are downloaded from the ImmPort database and are already preprocessed. The data includes three parts, meta-data, CyTOF data, and marker names. \n",
    "\n",
    "* The **meta-data** contains the sample level information, including the study accession number for each sample and the ground truth of CMV infection. It is stored as a pandas data frame. \n",
    "* The **CyTOF data** contains the single-cell profile of 27 markers. They are arcsinh transformed data from raw FCS files. It is stored in a three-dimensional numpy array. The dimension of the numpy array are 472 samples x 10000 cells x 27 markers. \n",
    "* The **marker names** contain the name of the markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Figure1.png', 'Final_weights.hdf5', 'header.png']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Data:\n",
      "--2019-06-13 19:10:11--  https://storage.googleapis.com/public-files-900/Zicheng_Tutorial/allData.obj\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 216.58.195.80, 2607:f8b0:4005:807::2010\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|216.58.195.80|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2019-06-13 19:10:11 ERROR 404: Not Found.\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/allData.obj'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c10bf6337c1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mallData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Data/allData.obj\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmetaData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"metaData\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mcytoData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mallData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cytoData\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/allData.obj'"
     ]
    }
   ],
   "source": [
    "##### Step 2: load data #####\n",
    "\n",
    "#Download data\n",
    "tutorial_files = ! ls Data\n",
    "if \"allData.obj\" not in tutorial_files:\n",
    "    print(\"Downloading Data:\")\n",
    "    ! wget https://storage.googleapis.com/public-files-900/Zicheng_Tutorial/allData.obj -P ./Data\n",
    "        \n",
    "#load data\n",
    "allData = pickle.load( open( \"Data/allData.obj\", \"rb\" ) )\n",
    "metaData = allData[\"metaData\"]\n",
    "cytoData = allData[\"cytoData\"]\n",
    "markerNames = allData[\"markerNames\"]\n",
    "\n",
    "print(\"First 5 rows of metaData: \")\n",
    "display(metaData.head())\n",
    "\n",
    "print(\"Dimensions of cytoData: \",cytoData.shape)\n",
    "print(\"Names of the 27 makers: \\n\",markerNames.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Split data into training, validation and testing sets\n",
    "Now, lets split the data into training, validation, and testing sets. The training data is used to train the deep learning model. The validation dataset is used to select the best parameters for the model and to avoid overfitting. The test dataset is used to evaluate the performance of the final model.\n",
    "\n",
    "We will use samples from the study SDY515 as a validation set, samples from the study SDY519 as a testing set, and the rest of the samples as a training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Step 3: split train, validation and test######\n",
    "y = metaData.CMV_Ab.values\n",
    "x = cytoData\n",
    "\n",
    "train_id = (metaData.study_accession.isin([\"SDY515\",\"SDY519\"])==False)\n",
    "valid_id = metaData.study_accession==\"SDY515\"\n",
    "test_id = metaData.study_accession ==\"SDY519\"\n",
    "\n",
    "x_train = x[train_id]; y_train = y[train_id]\n",
    "x_valid = x[valid_id]; y_valid = y[valid_id]\n",
    "x_test = x[test_id]; y_test = y[test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define deep learning model\n",
    "We will use a customized convolution neural network (CNN) to analyze the CyTOF data. For each sample, the CyTOF data is a matrix with rows as cells and columns as markers. It is crucial to notice that the order of cells is arbitrary in CyTOF data. For example, both matrix 1 and matrix 2 profiles the same sample in Figure 1A, even though they have different orders of rows. \n",
    "\n",
    "![alt text](Data/Figure1.png)\n",
    "\n",
    "\n",
    "Based on the characteristics of the CyTOF data, we design a CNN model to predict sample level information using the raw CyTOF data as input. The model contains six layers: input layer, first and second convolution layer, pooling layer, dense layer, and output layer. \n",
    "\n",
    "* The **input layer** receives the CyTOF data matrix. \n",
    "\n",
    "* The **first convolution layer** uses three filters to scan each row of the CyTOF data. This layer extracts relevant information from the cell marker profile of each cell. \n",
    "\n",
    "* The **second convolution layer** uses three filters to scan each row of the first layer's output. Each filter combines information from the first layer for each cell. \n",
    "\n",
    "* The **pooling layers** averages the outputs of the second convolution layer. The purpose is to aggregate the cell level information into sample level information. \n",
    "\n",
    "* The **dense layer** further extracts information from the pooling layer. \n",
    "\n",
    "* The **output layer** uses logistic regression to report the probability of CMV infection for each sample. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Step 4: define model #####\n",
    "\n",
    "# input\n",
    "model_input = Input(shape=x_train[0].shape)\n",
    "\n",
    "# first convolution layer\n",
    "model_output = Conv2D(3, kernel_size=(1, x_train.shape[2]),\n",
    "                 activation=None)(model_input)\n",
    "model_output = BatchNormalization()(model_output)\n",
    "model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "# sceond convolution layer\n",
    "model_output = Conv2D(3, (1, 1), activation=None)(model_output)\n",
    "model_output = BatchNormalization()(model_output)\n",
    "model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "# pooling layer\n",
    "model_output = AveragePooling2D(pool_size=(x_train.shape[1], 1))(model_output)\n",
    "model_output = Flatten()(model_output)\n",
    "\n",
    "# Dense layer\n",
    "model_output = Dense(3, activation=None)(model_output)\n",
    "model_output = BatchNormalization()(model_output)\n",
    "model_output = Activation(\"relu\")(model_output)\n",
    "\n",
    "# output layer\n",
    "model_output = Dense(1, activation=None)(model_output)\n",
    "model_output = BatchNormalization()(model_output)\n",
    "model_output = Activation(\"sigmoid\")(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fit the model\n",
    "In this step, we will use the training data to fit the model. We will use the Adam algorithm, which is an extension of the gradient descent method to train our model. Adam algorithm will search the model space step by step (epochs) until the optimal model is identified. At each step, we will use validation data to evaluate the performance of the model. The best model will be saved. \n",
    "\n",
    "To save time, we will only run the first 150 epochs as a demonstration. We will load the final model from a save file (Final_weights.hdf5) for the following analysis steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Step 5: Fit model #####\n",
    "\n",
    "# specify input and output\n",
    "model = Model(inputs=[model_input],\n",
    "              outputs=model_output)\n",
    "\n",
    "# define loss function and optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=Adam(lr=0.0001),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# save the best performing model using validation result\n",
    "checkpointer = ModelCheckpoint(filepath='Result/saved_weights.hdf5', \n",
    "                               monitor='val_loss', verbose=0, \n",
    "                               save_best_only=True)\n",
    "\n",
    "# model training\n",
    "model.fit([x_train], y_train,\n",
    "          batch_size=60,\n",
    "          epochs=150, # increase the epochs to 500 to fully train the model\n",
    "          verbose=1,\n",
    "          callbacks=[checkpointer],\n",
    "          validation_data=([x_valid], y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Plot the training history\n",
    "We can view the training history of the model by plotting the performance (value of the loss function) for training and validation data in each epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Step 6: plot train and validation loss #####\n",
    "plt.plot(model.history.history['loss'])\n",
    "plt.plot(model.history.history['val_loss'])\n",
    "plt.title('model train vs validation loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Evaluate the performance using test data\n",
    "We now use the test data, which has not been touched so far, to evaluate the performance of the final model. We will draw a Receiver Operator Characteristic(ROC) Curve and use Area Under the Curve (AUC) to measure performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Step 7: test the final model #####\n",
    "\n",
    "# load final model\n",
    "final_model = load_model('Data/Final_weights.hdf5')\n",
    "\n",
    "# generate ROC and AUC\n",
    "y_scores = final_model.predict([x_test])\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('AUC = {0:.2f}'.format(roc_auc))\n",
    "plt.show()\n",
    "\n",
    "# Plot predicted Probability of CMV infection\n",
    "pred = pd.DataFrame({\"Probability\":y_scores[:,0], \"CMV\":(y_test==1.0)})\n",
    "sns.stripplot(y = \"Probability\", x = \"CMV\",data=pred);\n",
    "sns.boxplot(y = \"Probability\", x = \"CMV\",data=pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
